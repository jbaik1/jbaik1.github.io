@misc{huang2024causalgraphodecontinuous,
      title={Causal Graph ODE: Continuous Treatment Effect Modeling in Multi-agent Dynamical Systems}, 
      author={Zijie Huang and Jeehyun Hwang and Junkai Zhang and Jinwoo Baik and Weitong Zhang and Dominik Wodarz and Yizhou Sun and Quanquan Gu and Wei Wang},
      year={2024},
      eprint={2403.00178},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.00178}, 
}


@article{10.1121/1.5121717,
    author = {Huilgol, Shreya and Baik, Jinwoo and Shattuck-Hufnagel, Stefanie},
    title = {A framework for labeling speech with acoustic cues to linguistic distinctive features},
    journal = {The Journal of the Acoustical Society of America},
    volume = {146},
    number = {2},
    pages = {EL184-EL190},
    year = {2019},
    month = {08},
    abstract = {Acoustic cues are characteristic patterns in the speech signal that provide lexical, prosodic, or additional information, such as speaker identity. In particular, acoustic cues related to linguistic distinctive features can be extracted and marked from the speech signal. These acoustic cues can be used to infer the intended underlying phoneme sequence in an utterance. This study describes a framework for labeling acoustic cues in speech, including a suite of canonical cue prediction algorithms that facilitates manual labeling and provides a standard for analyzing variations in the surface realizations. A brief examination of subsets of annotated speech data shows that labeling acoustic cues opens the possibility of detailed analyses of cue modification patterns in speech.},
    issn = {0001-4966},
    doi = {10.1121/1.5121717},
    url = {https://doi.org/10.1121/1.5121717},
    eprint = {https://pubs.aip.org/asa/jasa/article-pdf/146/2/EL184/14718745/el184\_1\_online.pdf},
}

@article {Afifa2022.03.01.482161,
	author = {Afifa, Umaima and Carmona, Javier and Dinh, Amy and Espino, Diego and McCarthy, Trevor and Ta, Brian and Wilson, Patrick and Asdell, Benjamin and Baik, Jinwoo and Biju, Archana and Chung, Sonia and Dao, Christopher and Diamond, Mark and Doust, Saba and East, Angela and Espino, Diego and Fleiszig-Evans, Kailey and Franco, Adrian and Garibay-Gutierrez, Anthony and Guha, Aparajeeta and Gunturu, Roshan and Handley, Luke and Honore, Christina and Kannan, Abinav and Khoo, Jared and Khosla, Mira and Kittur, Chandan and Kwon, Alexandra and Lee, Jessica and Lwe, Nicholas and Mayer, Mylan and Mills, Elizabeth and Pineda, Delilah and Pourebrahim, Pasha and Rajacich, Jacob and Rizvi, Shan and Rosales, Liliana and Schummer, Leonard and Sefkow, Conor and Stangel, Alexander and Ta, Cindy and Ta, Ivy and Tong, Natalie and Tsujimoto, Kyle and Vu, Alyssa and Wang, Henry and Yares, Amanda and Yamaguchi, Natsuko and Yoon, Ki Woong and Yu, Shuyi and Blaisdell, Aaron P. and Arisaka, Katsushi},
	title = {Visual Perception of 3D Space and Shape in Time - Part I: 2D Space Perception by 2D Linear Translation},
	elocation-id = {2022.03.01.482161},
	year = {2022},
	doi = {10.1101/2022.03.01.482161},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Visual perception plays a critical role in navigating space and extracting useful semantic information crucial to survival. To identify distant landmarks, we constantly shift gaze vectors through saccades, while still maintaining the visual perception of stable allocentric space. How can we sustain stable allocentric space so effortlessly? To solve this question, we have developed a new concept of NHT (Neural Holography Tomography). This model states that retinotopy is invisible (not available to consciousness) and must be converted to a time code by traveling alpha brainwaves to perceive objects consciously. According to this framework, if identical alpha phases are continually assigned to a landmark, we perceive its exact and consistent allocentric location.To test this hypothesis, we designed reaction time (RT) experiments to observe evidence of the predicted space-to-time conversion. Various visual stimuli were generated at a wide range of eccentricities either on a large TV (up to 40{\textdegree}) or by LED strips on a hemispherical dome (up to 60{\textdegree}). Participants were instructed to report the observed patterns promptly under either covert (no eye movement) or overt (with eye movement) conditions. As predicted, stimuli presented at the center of fixation always produced the fastest RTs. The additional RT delay was precisely proportional to the eccentricity of the peripheral stimulus presentation. Furthermore, both covert and overt attention protocols created the same RT delays, and trajectories of saccadic eye motions were in parallel to the overt RT vs. eccentricity. These findings strongly support our NHT model, in which the observed RT-eccentricity dependence is indicative of the spatiotemporal conversion required for maintaining a stable allocentric frame of reference. That is, we perceive space by time.Competing Interest StatementThe authors have declared no competing interest.},
	URL = {https://www.biorxiv.org/content/early/2022/03/05/2022.03.01.482161},
	eprint = {https://www.biorxiv.org/content/early/2022/03/05/2022.03.01.482161.full.pdf},
	journal = {bioRxiv}
}
